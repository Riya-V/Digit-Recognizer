{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.274918\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.025267\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.020577\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.019472\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.015545\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.017731\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.013951\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.007282\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.011480\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.011476\n",
      "\n",
      "Test set: Average loss: 0.1049, Accuracy: 9394/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010957\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.008090\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.006146\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.011546\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007521\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.006160\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.008334\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.004962\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.010122\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.007099\n",
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 9536/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.009445\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.005759\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.005486\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003317\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.006060\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.003807\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.007417\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014120\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.005981\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008598\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9588/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005701\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.009952\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.003682\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005722\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003520\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006616\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.005776\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.009022\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004865\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.005905\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 9652/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010073\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.005446\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004753\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.008483\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002852\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.004889\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.007897\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005840\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.009440\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003051\n",
      "\n",
      "Test set: Average loss: 0.0545, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004560\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.007206\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.003539\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005356\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001615\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006138\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.007906\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002370\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.005132\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002808\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002659\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005936\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004451\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001209\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006464\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001330\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.007383\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001020\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004539\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001471\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005405\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002072\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001752\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000844\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002790\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006174\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.003062\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.003011\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003347\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003234\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002738\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004390\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.006352\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000903\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001409\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001507\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.004618\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002049\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004405\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000863\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006523\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001108\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000841\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002080\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002373\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002950\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.008099\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002576\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002516\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001436\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9740/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):  # Constructor\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 2 Layers\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "        # xavier_initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - shape = [64, 784]\n",
    "        # Layer 1\n",
    "        l1 = self.fc1(x)  # l1 - shape = [64, 100]\n",
    "\n",
    "        # Activation 1\n",
    "        l1_a1 = torch.sigmoid(l1)  # l1_a1 - shape = [64, 100]\n",
    "\n",
    "        # Layer 2\n",
    "        l2 = self.fc2(l1_a1)  # l2 - shape = [64, 10]\n",
    "\n",
    "        # Activation 2\n",
    "        l2_a2 = torch.sigmoid(l2)  # l2_a2 - shape = [64, 10]\n",
    "        \n",
    "        return l2_a2\n",
    "\n",
    "\n",
    "def train(model, use_cuda, train_loader, optimizer, epoch):\n",
    "\n",
    "    model.train()  # Tell the model to prepare for training\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Get the batch\n",
    "\n",
    "        # Converting the target to one-hot-encoding from categorical encoding\n",
    "        # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [64, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1\n",
    "\n",
    "        data = data.view([data.shape[0], 784])\n",
    "\n",
    "        if use_cuda:\n",
    "            data, y_onehot = data.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "        optimizer.zero_grad()  # Setting the cumulative gradients to 0\n",
    "        output = model(data)  # Forward pass through the model\n",
    "        loss = torch.mean((output - y_onehot)**2)  # Calculating the loss\n",
    "        loss.backward()  # Calculating the gradients of the model. Note that the model has not yet been updated.\n",
    "        optimizer.step()  # Updating the model parameters. Note that this does not remove the stored gradients!\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, use_cuda, test_loader):\n",
    "\n",
    "    model.eval()  # Tell the model to prepare for testing or evaluation\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Tell the model that gradients need not be calculated\n",
    "        for data, target in test_loader:  # Get the batch\n",
    "\n",
    "            # Converting the target to one-hot-encoding from categorical encoding\n",
    "            # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "            y_onehot = torch.zeros([target.shape[0], 10])\n",
    "            y_onehot[range(target.shape[0]), target] = 1\n",
    "            data = data.view([data.shape[0], 784])\n",
    "\n",
    "            if use_cuda:\n",
    "                data, target, y_onehot = data.cuda(), target.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "            # argmax([0.1, 0.2, 0.9, 0.4]) => 2\n",
    "            # output - shape = [1000, 10], argmax(dim=1) => [1000]\n",
    "            output = model(data)  # Forward pass\n",
    "            test_loss += torch.sum((output - y_onehot)**2)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the maximum output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Get total number of correct samples\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Accuracy = Total Correct / Total Samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def seed(seed_value):\n",
    "\n",
    "    # This removes randomness, makes everything deterministic\n",
    "\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    use_cuda = False  # Set it to False if you are using a CPU\n",
    "    # Colab And Kaggle\n",
    "\n",
    "    seed(0)  # Used to fix randomness in the code! Very important!\n",
    "\n",
    "    # Convert the dataset to tensor and subtract the mean and divide by standard\n",
    "    # deviation. Why? So that neurons don't saturate!\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # ((mean), (std))    (x - mean)/std\n",
    "    ])\n",
    "    # [a1, a2, a3] / [b1, b2, b3] = [a1/b1, a2/b2, a3/b3]\n",
    "\n",
    "    # Wx + b =~ b\n",
    "\n",
    "    dataset1 = datasets.MNIST('./data', train=True, download=True, transform=transform)  # Get the train dataset\n",
    "    dataset2 = datasets.MNIST('./data', train=False, transform=transform)  # Get the test dataset\n",
    "\n",
    "    # Get the train data-loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, num_workers=6, shuffle=True, batch_size=64)\n",
    "    # Get the test data-loader\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, num_workers=6, shuffle=False, batch_size=1000)\n",
    "\n",
    "    model = Net()  # Get the model\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Put the model weights on GPU\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=10)  # Choose the optimizer and the set the learning rate\n",
    "\n",
    "    for epoch in range(1, 10 + 1):\n",
    "        train(model, use_cuda, train_loader, optimizer, epoch)  # Train the network\n",
    "        test(model, use_cuda, test_loader)  # Test the network\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist.pt\")\n",
    "\n",
    "    model.load_state_dict(torch.load('mnist.pt'))\n",
    "\n",
    "    # Loading a saved model - model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.274918\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.025267\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.020577\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.019472\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.015545\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.017731\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.013951\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.007282\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.011480\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.011476\n",
      "\n",
      "Test set: Average loss: 0.1049, Accuracy: 9394/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010957\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.008090\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.006146\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.011546\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007521\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.006160\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.008334\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.004962\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.010122\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.007099\n",
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 9536/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.009445\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.005759\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.005486\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003317\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.006060\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.003807\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.007417\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014120\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.005981\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008598\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9588/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005701\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.009952\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.003682\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005722\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003520\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006616\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.005776\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.009022\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004865\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.005905\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 9652/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010073\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.005446\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004753\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.008483\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002852\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.004889\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.007897\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005840\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.009440\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003051\n",
      "\n",
      "Test set: Average loss: 0.0545, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004560\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.007206\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.003539\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005356\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001615\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006138\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.007906\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002370\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.005132\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002808\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002659\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005936\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004451\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001209\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006464\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001330\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.007383\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001020\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004539\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001471\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005405\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002072\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001752\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000844\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002790\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006174\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.003062\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.003011\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003347\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003234\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002738\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004390\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.006352\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000903\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001409\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001507\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.004618\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002049\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004405\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000863\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006523\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001108\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000841\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002080\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002373\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002950\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.008099\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002576\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002516\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001436\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9740/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):  # Constructor\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 2 Layers\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "        # xavier_initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - shape = [64, 784]\n",
    "        # Layer 1\n",
    "        l1 = self.fc1(x)  # l1 - shape = [64, 100]\n",
    "\n",
    "        # Activation 1\n",
    "        l1_a1 = torch.sigmoid(l1)  # l1_a1 - shape = [64, 100]\n",
    "\n",
    "        # Layer 2\n",
    "        l2 = self.fc2(l1_a1)  # l2 - shape = [64, 10]\n",
    "\n",
    "        # Activation 2\n",
    "        l2_a2 = torch.sigmoid(l2)  # l2_a2 - shape = [64, 10]\n",
    "        \n",
    "        return l2_a2\n",
    "\n",
    "\n",
    "def train(model, use_cuda, train_loader, optimizer, epoch):\n",
    "\n",
    "    model.train()  # Tell the model to prepare for training\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Get the batch\n",
    "\n",
    "        # Converting the target to one-hot-encoding from categorical encoding\n",
    "        # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [64, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1\n",
    "\n",
    "        data = data.view([data.shape[0], 784])\n",
    "\n",
    "        if use_cuda:\n",
    "            data, y_onehot = data.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "        optimizer.zero_grad()  # Setting the cumulative gradients to 0\n",
    "        output = model(data)  # Forward pass through the model\n",
    "        loss = torch.mean((output - y_onehot)**2)  # Calculating the loss\n",
    "        loss.backward()  # Calculating the gradients of the model. Note that the model has not yet been updated.\n",
    "        optimizer.step()  # Updating the model parameters. Note that this does not remove the stored gradients!\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, use_cuda, test_loader):\n",
    "\n",
    "    model.eval()  # Tell the model to prepare for testing or evaluation\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Tell the model that gradients need not be calculated\n",
    "        for data, target in test_loader:  # Get the batch\n",
    "\n",
    "            # Converting the target to one-hot-encoding from categorical encoding\n",
    "            # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "            y_onehot = torch.zeros([target.shape[0], 10])\n",
    "            y_onehot[range(target.shape[0]), target] = 1\n",
    "            data = data.view([data.shape[0], 784])\n",
    "\n",
    "            if use_cuda:\n",
    "                data, target, y_onehot = data.cuda(), target.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "            # argmax([0.1, 0.2, 0.9, 0.4]) => 2\n",
    "            # output - shape = [1000, 10], argmax(dim=1) => [1000]\n",
    "            output = model(data)  # Forward pass\n",
    "            test_loss += torch.sum((output - y_onehot)**2)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the maximum output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Get total number of correct samples\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Accuracy = Total Correct / Total Samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def seed(seed_value):\n",
    "\n",
    "    # This removes randomness, makes everything deterministic\n",
    "\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    use_cuda = False  # Set it to False if you are using a CPU\n",
    "    # Colab And Kaggle\n",
    "\n",
    "    seed(0)  # Used to fix randomness in the code! Very important!\n",
    "\n",
    "    # Convert the dataset to tensor and subtract the mean and divide by standard\n",
    "    # deviation. Why? So that neurons don't saturate!\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # ((mean), (std))    (x - mean)/std\n",
    "    ])\n",
    "    # [a1, a2, a3] / [b1, b2, b3] = [a1/b1, a2/b2, a3/b3]\n",
    "\n",
    "    # Wx + b =~ b\n",
    "\n",
    "    dataset1 = datasets.MNIST('./data', train=True, download=True, transform=transform)  # Get the train dataset\n",
    "    dataset2 = datasets.MNIST('./data', train=False, transform=transform)  # Get the test dataset\n",
    "\n",
    "    # Get the train data-loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, num_workers=6, shuffle=True, batch_size=64)\n",
    "    # Get the test data-loader\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, num_workers=6, shuffle=False, batch_size=1000)\n",
    "\n",
    "    model = Net()  # Get the model\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Put the model weights on GPU\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=10)  # Choose the optimizer and the set the learning rate\n",
    "\n",
    "    for epoch in range(1, 10 + 1):\n",
    "        train(model, use_cuda, train_loader, optimizer, epoch)  # Train the network\n",
    "        test(model, use_cuda, test_loader)  # Test the network\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist.pt\")\n",
    "\n",
    "    model.load_state_dict(torch.load('mnist.pt'))\n",
    "\n",
    "    # Loading a saved model - model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.274918\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.025267\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.020577\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.019472\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.015545\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.017731\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.013951\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.007282\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.011480\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.011476\n",
      "\n",
      "Test set: Average loss: 0.1049, Accuracy: 9394/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010957\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.008090\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.006146\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.011546\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007521\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.006160\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.008334\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.004962\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.010122\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.007099\n",
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 9536/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.009445\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.005759\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.005486\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003317\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.006060\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.003807\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.007417\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014120\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.005981\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008598\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9588/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005701\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.009952\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.003682\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005722\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003520\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006616\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.005776\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.009022\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004865\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.005905\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 9652/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010073\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.005446\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004753\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.008483\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002852\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.004889\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.007897\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005840\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.009440\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003051\n",
      "\n",
      "Test set: Average loss: 0.0545, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004560\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.007206\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.003539\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005356\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001615\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006138\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.007906\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002370\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.005132\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002808\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002659\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005936\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004451\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001209\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006464\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001330\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.007383\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001020\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004539\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001471\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005405\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002072\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001752\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000844\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002790\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006174\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.003062\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.003011\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003347\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003234\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002738\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004390\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.006352\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000903\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001409\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001507\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.004618\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002049\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004405\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000863\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006523\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001108\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000841\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002080\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002373\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002950\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.008099\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002576\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002516\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001436\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9740/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):  # Constructor\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 2 Layers\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "        # xavier_initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - shape = [64, 784]\n",
    "        # Layer 1\n",
    "        l1 = self.fc1(x)  # l1 - shape = [64, 100]\n",
    "\n",
    "        # Activation 1\n",
    "        l1_a1 = torch.sigmoid(l1)  # l1_a1 - shape = [64, 100]\n",
    "\n",
    "        # Layer 2\n",
    "        l2 = self.fc2(l1_a1)  # l2 - shape = [64, 10]\n",
    "\n",
    "        # Activation 2\n",
    "        l2_a2 = torch.sigmoid(l2)  # l2_a2 - shape = [64, 10]\n",
    "        \n",
    "        return l2_a2\n",
    "\n",
    "\n",
    "def train(model, use_cuda, train_loader, optimizer, epoch):\n",
    "\n",
    "    model.train()  # Tell the model to prepare for training\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Get the batch\n",
    "\n",
    "        # Converting the target to one-hot-encoding from categorical encoding\n",
    "        # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [64, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1\n",
    "\n",
    "        data = data.view([data.shape[0], 784])\n",
    "\n",
    "        if use_cuda:\n",
    "            data, y_onehot = data.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "        optimizer.zero_grad()  # Setting the cumulative gradients to 0\n",
    "        output = model(data)  # Forward pass through the model\n",
    "        loss = torch.mean((output - y_onehot)**2)  # Calculating the loss\n",
    "        loss.backward()  # Calculating the gradients of the model. Note that the model has not yet been updated.\n",
    "        optimizer.step()  # Updating the model parameters. Note that this does not remove the stored gradients!\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, use_cuda, test_loader):\n",
    "\n",
    "    model.eval()  # Tell the model to prepare for testing or evaluation\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Tell the model that gradients need not be calculated\n",
    "        for data, target in test_loader:  # Get the batch\n",
    "\n",
    "            # Converting the target to one-hot-encoding from categorical encoding\n",
    "            # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "            y_onehot = torch.zeros([target.shape[0], 10])\n",
    "            y_onehot[range(target.shape[0]), target] = 1\n",
    "            data = data.view([data.shape[0], 784])\n",
    "\n",
    "            if use_cuda:\n",
    "                data, target, y_onehot = data.cuda(), target.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "            # argmax([0.1, 0.2, 0.9, 0.4]) => 2\n",
    "            # output - shape = [1000, 10], argmax(dim=1) => [1000]\n",
    "            output = model(data)  # Forward pass\n",
    "            test_loss += torch.sum((output - y_onehot)**2)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the maximum output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Get total number of correct samples\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Accuracy = Total Correct / Total Samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def seed(seed_value):\n",
    "\n",
    "    # This removes randomness, makes everything deterministic\n",
    "\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    use_cuda = False  # Set it to False if you are using a CPU\n",
    "    # Colab And Kaggle\n",
    "\n",
    "    seed(0)  # Used to fix randomness in the code! Very important!\n",
    "\n",
    "    # Convert the dataset to tensor and subtract the mean and divide by standard\n",
    "    # deviation. Why? So that neurons don't saturate!\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # ((mean), (std))    (x - mean)/std\n",
    "    ])\n",
    "    # [a1, a2, a3] / [b1, b2, b3] = [a1/b1, a2/b2, a3/b3]\n",
    "\n",
    "    # Wx + b =~ b\n",
    "\n",
    "    dataset1 = datasets.MNIST('./data', train=True, download=True, transform=transform)  # Get the train dataset\n",
    "    dataset2 = datasets.MNIST('./data', train=False, transform=transform)  # Get the test dataset\n",
    "\n",
    "    # Get the train data-loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, num_workers=6, shuffle=True, batch_size=64)\n",
    "    # Get the test data-loader\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, num_workers=6, shuffle=False, batch_size=1000)\n",
    "\n",
    "    model = Net()  # Get the model\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Put the model weights on GPU\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=10)  # Choose the optimizer and the set the learning rate\n",
    "\n",
    "    for epoch in range(1, 10 + 1):\n",
    "        train(model, use_cuda, train_loader, optimizer, epoch)  # Train the network\n",
    "        test(model, use_cuda, test_loader)  # Test the network\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist.pt\")\n",
    "\n",
    "    model.load_state_dict(torch.load('mnist.pt'))\n",
    "\n",
    "    # Loading a saved model - model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.274918\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.025267\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.020577\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.019472\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.015545\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.017731\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.013951\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.007282\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.011480\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.011476\n",
      "\n",
      "Test set: Average loss: 0.1049, Accuracy: 9394/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010957\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.008090\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.006146\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.011546\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007521\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.006160\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.008334\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.004962\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.010122\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.007099\n",
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 9536/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.009445\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.005759\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.005486\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003317\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.006060\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.003807\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.007417\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014120\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.005981\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008598\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9588/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005701\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.009952\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.003682\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005722\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003520\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006616\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.005776\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.009022\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004865\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.005905\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 9652/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010073\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.005446\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004753\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.008483\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002852\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.004889\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.007897\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005840\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.009440\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003051\n",
      "\n",
      "Test set: Average loss: 0.0545, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004560\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.007206\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.003539\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005356\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001615\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006138\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.007906\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002370\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.005132\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002808\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002659\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005936\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004451\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001209\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006464\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001330\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.007383\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001020\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004539\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001471\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005405\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002072\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001752\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000844\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002790\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006174\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.003062\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.003011\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003347\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003234\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002738\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004390\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.006352\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000903\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001409\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001507\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.004618\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002049\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004405\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000863\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006523\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001108\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000841\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002080\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002373\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002950\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.008099\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002576\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002516\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001436\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9740/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):  # Constructor\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 2 Layers\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "        # xavier_initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - shape = [64, 784]\n",
    "        # Layer 1\n",
    "        l1 = self.fc1(x)  # l1 - shape = [64, 100]\n",
    "\n",
    "        # Activation 1\n",
    "        l1_a1 = torch.sigmoid(l1)  # l1_a1 - shape = [64, 100]\n",
    "\n",
    "        # Layer 2\n",
    "        l2 = self.fc2(l1_a1)  # l2 - shape = [64, 10]\n",
    "\n",
    "        # Activation 2\n",
    "        l2_a2 = torch.sigmoid(l2)  # l2_a2 - shape = [64, 10]\n",
    "        \n",
    "        return l2_a2\n",
    "\n",
    "\n",
    "def train(model, use_cuda, train_loader, optimizer, epoch):\n",
    "\n",
    "    model.train()  # Tell the model to prepare for training\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Get the batch\n",
    "\n",
    "        # Converting the target to one-hot-encoding from categorical encoding\n",
    "        # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [64, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1\n",
    "\n",
    "        data = data.view([data.shape[0], 784])\n",
    "\n",
    "        if use_cuda:\n",
    "            data, y_onehot = data.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "        optimizer.zero_grad()  # Setting the cumulative gradients to 0\n",
    "        output = model(data)  # Forward pass through the model\n",
    "        loss = torch.mean((output - y_onehot)**2)  # Calculating the loss\n",
    "        loss.backward()  # Calculating the gradients of the model. Note that the model has not yet been updated.\n",
    "        optimizer.step()  # Updating the model parameters. Note that this does not remove the stored gradients!\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, use_cuda, test_loader):\n",
    "\n",
    "    model.eval()  # Tell the model to prepare for testing or evaluation\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Tell the model that gradients need not be calculated\n",
    "        for data, target in test_loader:  # Get the batch\n",
    "\n",
    "            # Converting the target to one-hot-encoding from categorical encoding\n",
    "            # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "            y_onehot = torch.zeros([target.shape[0], 10])\n",
    "            y_onehot[range(target.shape[0]), target] = 1\n",
    "            data = data.view([data.shape[0], 784])\n",
    "\n",
    "            if use_cuda:\n",
    "                data, target, y_onehot = data.cuda(), target.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "            # argmax([0.1, 0.2, 0.9, 0.4]) => 2\n",
    "            # output - shape = [1000, 10], argmax(dim=1) => [1000]\n",
    "            output = model(data)  # Forward pass\n",
    "            test_loss += torch.sum((output - y_onehot)**2)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the maximum output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Get total number of correct samples\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Accuracy = Total Correct / Total Samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def seed(seed_value):\n",
    "\n",
    "    # This removes randomness, makes everything deterministic\n",
    "\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    use_cuda = False  # Set it to False if you are using a CPU\n",
    "    # Colab And Kaggle\n",
    "\n",
    "    seed(0)  # Used to fix randomness in the code! Very important!\n",
    "\n",
    "    # Convert the dataset to tensor and subtract the mean and divide by standard\n",
    "    # deviation. Why? So that neurons don't saturate!\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # ((mean), (std))    (x - mean)/std\n",
    "    ])\n",
    "    # [a1, a2, a3] / [b1, b2, b3] = [a1/b1, a2/b2, a3/b3]\n",
    "\n",
    "    # Wx + b =~ b\n",
    "\n",
    "    dataset1 = datasets.MNIST('./data', train=True, download=True, transform=transform)  # Get the train dataset\n",
    "    dataset2 = datasets.MNIST('./data', train=False, transform=transform)  # Get the test dataset\n",
    "\n",
    "    # Get the train data-loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, num_workers=6, shuffle=True, batch_size=64)\n",
    "    # Get the test data-loader\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, num_workers=6, shuffle=False, batch_size=1000)\n",
    "\n",
    "    model = Net()  # Get the model\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Put the model weights on GPU\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=10)  # Choose the optimizer and the set the learning rate\n",
    "\n",
    "    for epoch in range(1, 10 + 1):\n",
    "        train(model, use_cuda, train_loader, optimizer, epoch)  # Train the network\n",
    "        test(model, use_cuda, test_loader)  # Test the network\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist.pt\")\n",
    "\n",
    "    model.load_state_dict(torch.load('mnist.pt'))\n",
    "\n",
    "    # Loading a saved model - model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.274918\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.025267\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.020577\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.019472\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.015545\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.017731\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.013951\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.007282\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.011480\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.011476\n",
      "\n",
      "Test set: Average loss: 0.1049, Accuracy: 9394/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010957\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.008090\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.006146\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.011546\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007521\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.006160\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.008334\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.004962\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.010122\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.007099\n",
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 9536/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.009445\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.005759\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.005486\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003317\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.006060\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.003807\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.007417\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014120\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.005981\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008598\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9588/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005701\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.009952\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.003682\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005722\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003520\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006616\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.005776\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.009022\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004865\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.005905\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 9652/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010073\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.005446\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004753\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.008483\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002852\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.004889\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.007897\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005840\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.009440\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003051\n",
      "\n",
      "Test set: Average loss: 0.0545, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004560\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.007206\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.003539\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005356\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001615\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006138\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.007906\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002370\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.005132\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002808\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002659\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005936\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004451\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001209\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006464\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001330\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.007383\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001020\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004539\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001471\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005405\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002072\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001752\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000844\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002790\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006174\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.003062\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.003011\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003347\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003234\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002738\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004390\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.006352\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000903\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001409\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001507\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.004618\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002049\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004405\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000863\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006523\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001108\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000841\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002080\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002373\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002950\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.008099\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002576\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002516\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001436\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9740/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):  # Constructor\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 2 Layers\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "        # xavier_initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - shape = [64, 784]\n",
    "        # Layer 1\n",
    "        l1 = self.fc1(x)  # l1 - shape = [64, 100]\n",
    "\n",
    "        # Activation 1\n",
    "        l1_a1 = torch.sigmoid(l1)  # l1_a1 - shape = [64, 100]\n",
    "\n",
    "        # Layer 2\n",
    "        l2 = self.fc2(l1_a1)  # l2 - shape = [64, 10]\n",
    "\n",
    "        # Activation 2\n",
    "        l2_a2 = torch.sigmoid(l2)  # l2_a2 - shape = [64, 10]\n",
    "        \n",
    "        return l2_a2\n",
    "\n",
    "\n",
    "def train(model, use_cuda, train_loader, optimizer, epoch):\n",
    "\n",
    "    model.train()  # Tell the model to prepare for training\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Get the batch\n",
    "\n",
    "        # Converting the target to one-hot-encoding from categorical encoding\n",
    "        # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [64, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1\n",
    "\n",
    "        data = data.view([data.shape[0], 784])\n",
    "\n",
    "        if use_cuda:\n",
    "            data, y_onehot = data.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "        optimizer.zero_grad()  # Setting the cumulative gradients to 0\n",
    "        output = model(data)  # Forward pass through the model\n",
    "        loss = torch.mean((output - y_onehot)**2)  # Calculating the loss\n",
    "        loss.backward()  # Calculating the gradients of the model. Note that the model has not yet been updated.\n",
    "        optimizer.step()  # Updating the model parameters. Note that this does not remove the stored gradients!\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, use_cuda, test_loader):\n",
    "\n",
    "    model.eval()  # Tell the model to prepare for testing or evaluation\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Tell the model that gradients need not be calculated\n",
    "        for data, target in test_loader:  # Get the batch\n",
    "\n",
    "            # Converting the target to one-hot-encoding from categorical encoding\n",
    "            # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "            y_onehot = torch.zeros([target.shape[0], 10])\n",
    "            y_onehot[range(target.shape[0]), target] = 1\n",
    "            data = data.view([data.shape[0], 784])\n",
    "\n",
    "            if use_cuda:\n",
    "                data, target, y_onehot = data.cuda(), target.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "            # argmax([0.1, 0.2, 0.9, 0.4]) => 2\n",
    "            # output - shape = [1000, 10], argmax(dim=1) => [1000]\n",
    "            output = model(data)  # Forward pass\n",
    "            test_loss += torch.sum((output - y_onehot)**2)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the maximum output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Get total number of correct samples\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Accuracy = Total Correct / Total Samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def seed(seed_value):\n",
    "\n",
    "    # This removes randomness, makes everything deterministic\n",
    "\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    use_cuda = False  # Set it to False if you are using a CPU\n",
    "    # Colab And Kaggle\n",
    "\n",
    "    seed(0)  # Used to fix randomness in the code! Very important!\n",
    "\n",
    "    # Convert the dataset to tensor and subtract the mean and divide by standard\n",
    "    # deviation. Why? So that neurons don't saturate!\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # ((mean), (std))    (x - mean)/std\n",
    "    ])\n",
    "    # [a1, a2, a3] / [b1, b2, b3] = [a1/b1, a2/b2, a3/b3]\n",
    "\n",
    "    # Wx + b =~ b\n",
    "\n",
    "    dataset1 = datasets.MNIST('./data', train=True, download=True, transform=transform)  # Get the train dataset\n",
    "    dataset2 = datasets.MNIST('./data', train=False, transform=transform)  # Get the test dataset\n",
    "\n",
    "    # Get the train data-loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, num_workers=6, shuffle=True, batch_size=64)\n",
    "    # Get the test data-loader\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, num_workers=6, shuffle=False, batch_size=1000)\n",
    "\n",
    "    model = Net()  # Get the model\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Put the model weights on GPU\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=10)  # Choose the optimizer and the set the learning rate\n",
    "\n",
    "    for epoch in range(1, 10 + 1):\n",
    "        train(model, use_cuda, train_loader, optimizer, epoch)  # Train the network\n",
    "        test(model, use_cuda, test_loader)  # Test the network\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist.pt\")\n",
    "\n",
    "    model.load_state_dict(torch.load('mnist.pt'))\n",
    "\n",
    "    # Loading a saved model - model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.274918\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.025267\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.020577\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.019472\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.015545\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.017731\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.013951\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.007282\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.011480\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.011476\n",
      "\n",
      "Test set: Average loss: 0.1049, Accuracy: 9394/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010957\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.008090\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.006146\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.011546\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007521\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.006160\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.008334\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.004962\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.010122\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.007099\n",
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 9536/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.009445\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.005759\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.005486\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003317\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.006060\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.003807\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.007417\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014120\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.005981\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008598\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9588/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005701\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.009952\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.003682\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005722\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003520\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006616\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.005776\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.009022\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004865\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.005905\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 9652/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010073\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.005446\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004753\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.008483\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002852\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.004889\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.007897\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005840\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.009440\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003051\n",
      "\n",
      "Test set: Average loss: 0.0545, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004560\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.007206\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.003539\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005356\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001615\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006138\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.007906\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002370\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.005132\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002808\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002659\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005936\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004451\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001209\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006464\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001330\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.007383\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001020\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004539\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001471\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005405\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002072\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001752\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000844\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002790\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006174\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.003062\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.003011\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003347\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003234\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002738\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004390\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.006352\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000903\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001409\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001507\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.004618\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002049\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004405\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000863\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006523\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001108\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000841\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002080\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002373\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002950\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.008099\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002576\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002516\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001436\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9740/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):  # Constructor\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 2 Layers\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "        # xavier_initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - shape = [64, 784]\n",
    "        # Layer 1\n",
    "        l1 = self.fc1(x)  # l1 - shape = [64, 100]\n",
    "\n",
    "        # Activation 1\n",
    "        l1_a1 = torch.sigmoid(l1)  # l1_a1 - shape = [64, 100]\n",
    "\n",
    "        # Layer 2\n",
    "        l2 = self.fc2(l1_a1)  # l2 - shape = [64, 10]\n",
    "\n",
    "        # Activation 2\n",
    "        l2_a2 = torch.sigmoid(l2)  # l2_a2 - shape = [64, 10]\n",
    "        \n",
    "        return l2_a2\n",
    "\n",
    "\n",
    "def train(model, use_cuda, train_loader, optimizer, epoch):\n",
    "\n",
    "    model.train()  # Tell the model to prepare for training\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Get the batch\n",
    "\n",
    "        # Converting the target to one-hot-encoding from categorical encoding\n",
    "        # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [64, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1\n",
    "\n",
    "        data = data.view([data.shape[0], 784])\n",
    "\n",
    "        if use_cuda:\n",
    "            data, y_onehot = data.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "        optimizer.zero_grad()  # Setting the cumulative gradients to 0\n",
    "        output = model(data)  # Forward pass through the model\n",
    "        loss = torch.mean((output - y_onehot)**2)  # Calculating the loss\n",
    "        loss.backward()  # Calculating the gradients of the model. Note that the model has not yet been updated.\n",
    "        optimizer.step()  # Updating the model parameters. Note that this does not remove the stored gradients!\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, use_cuda, test_loader):\n",
    "\n",
    "    model.eval()  # Tell the model to prepare for testing or evaluation\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Tell the model that gradients need not be calculated\n",
    "        for data, target in test_loader:  # Get the batch\n",
    "\n",
    "            # Converting the target to one-hot-encoding from categorical encoding\n",
    "            # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "            y_onehot = torch.zeros([target.shape[0], 10])\n",
    "            y_onehot[range(target.shape[0]), target] = 1\n",
    "            data = data.view([data.shape[0], 784])\n",
    "\n",
    "            if use_cuda:\n",
    "                data, target, y_onehot = data.cuda(), target.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "            # argmax([0.1, 0.2, 0.9, 0.4]) => 2\n",
    "            # output - shape = [1000, 10], argmax(dim=1) => [1000]\n",
    "            output = model(data)  # Forward pass\n",
    "            test_loss += torch.sum((output - y_onehot)**2)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the maximum output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Get total number of correct samples\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Accuracy = Total Correct / Total Samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def seed(seed_value):\n",
    "\n",
    "    # This removes randomness, makes everything deterministic\n",
    "\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    use_cuda = False  # Set it to False if you are using a CPU\n",
    "    # Colab And Kaggle\n",
    "\n",
    "    seed(0)  # Used to fix randomness in the code! Very important!\n",
    "\n",
    "    # Convert the dataset to tensor and subtract the mean and divide by standard\n",
    "    # deviation. Why? So that neurons don't saturate!\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # ((mean), (std))    (x - mean)/std\n",
    "    ])\n",
    "    # [a1, a2, a3] / [b1, b2, b3] = [a1/b1, a2/b2, a3/b3]\n",
    "\n",
    "    # Wx + b =~ b\n",
    "\n",
    "    dataset1 = datasets.MNIST('./data', train=True, download=True, transform=transform)  # Get the train dataset\n",
    "    dataset2 = datasets.MNIST('./data', train=False, transform=transform)  # Get the test dataset\n",
    "\n",
    "    # Get the train data-loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, num_workers=6, shuffle=True, batch_size=64)\n",
    "    # Get the test data-loader\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, num_workers=6, shuffle=False, batch_size=1000)\n",
    "\n",
    "    model = Net()  # Get the model\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Put the model weights on GPU\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=10)  # Choose the optimizer and the set the learning rate\n",
    "\n",
    "    for epoch in range(1, 10 + 1):\n",
    "        train(model, use_cuda, train_loader, optimizer, epoch)  # Train the network\n",
    "        test(model, use_cuda, test_loader)  # Test the network\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist.pt\")\n",
    "\n",
    "    model.load_state_dict(torch.load('mnist.pt'))\n",
    "\n",
    "    # Loading a saved model - model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.274918\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.025267\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.020577\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.019472\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.015545\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.017731\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.013951\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.007282\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.011480\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.011476\n",
      "\n",
      "Test set: Average loss: 0.1049, Accuracy: 9394/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010957\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.008090\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.006146\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.011546\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007521\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.006160\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.008334\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.004962\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.010122\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.007099\n",
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 9536/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.009445\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.005759\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.005486\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003317\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.006060\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.003807\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.007417\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014120\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.005981\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008598\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9588/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005701\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.009952\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.003682\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005722\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003520\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006616\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.005776\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.009022\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004865\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.005905\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 9652/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010073\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.005446\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004753\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.008483\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002852\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.004889\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.007897\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005840\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.009440\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003051\n",
      "\n",
      "Test set: Average loss: 0.0545, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004560\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.007206\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.003539\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005356\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001615\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006138\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.007906\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002370\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.005132\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002808\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002659\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005936\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004451\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001209\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006464\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001330\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.007383\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001020\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004539\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001471\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005405\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002072\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001752\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000844\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002790\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006174\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.003062\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.003011\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003347\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003234\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002738\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004390\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.006352\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000903\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001409\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001507\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.004618\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002049\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004405\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000863\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006523\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001108\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000841\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002080\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002373\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002950\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.008099\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002576\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002516\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001436\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9740/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):  # Constructor\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 2 Layers\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "        # xavier_initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - shape = [64, 784]\n",
    "        # Layer 1\n",
    "        l1 = self.fc1(x)  # l1 - shape = [64, 100]\n",
    "\n",
    "        # Activation 1\n",
    "        l1_a1 = torch.sigmoid(l1)  # l1_a1 - shape = [64, 100]\n",
    "\n",
    "        # Layer 2\n",
    "        l2 = self.fc2(l1_a1)  # l2 - shape = [64, 10]\n",
    "\n",
    "        # Activation 2\n",
    "        l2_a2 = torch.sigmoid(l2)  # l2_a2 - shape = [64, 10]\n",
    "        \n",
    "        return l2_a2\n",
    "\n",
    "\n",
    "def train(model, use_cuda, train_loader, optimizer, epoch):\n",
    "\n",
    "    model.train()  # Tell the model to prepare for training\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Get the batch\n",
    "\n",
    "        # Converting the target to one-hot-encoding from categorical encoding\n",
    "        # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [64, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1\n",
    "\n",
    "        data = data.view([data.shape[0], 784])\n",
    "\n",
    "        if use_cuda:\n",
    "            data, y_onehot = data.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "        optimizer.zero_grad()  # Setting the cumulative gradients to 0\n",
    "        output = model(data)  # Forward pass through the model\n",
    "        loss = torch.mean((output - y_onehot)**2)  # Calculating the loss\n",
    "        loss.backward()  # Calculating the gradients of the model. Note that the model has not yet been updated.\n",
    "        optimizer.step()  # Updating the model parameters. Note that this does not remove the stored gradients!\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, use_cuda, test_loader):\n",
    "\n",
    "    model.eval()  # Tell the model to prepare for testing or evaluation\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Tell the model that gradients need not be calculated\n",
    "        for data, target in test_loader:  # Get the batch\n",
    "\n",
    "            # Converting the target to one-hot-encoding from categorical encoding\n",
    "            # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "            y_onehot = torch.zeros([target.shape[0], 10])\n",
    "            y_onehot[range(target.shape[0]), target] = 1\n",
    "            data = data.view([data.shape[0], 784])\n",
    "\n",
    "            if use_cuda:\n",
    "                data, target, y_onehot = data.cuda(), target.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "            # argmax([0.1, 0.2, 0.9, 0.4]) => 2\n",
    "            # output - shape = [1000, 10], argmax(dim=1) => [1000]\n",
    "            output = model(data)  # Forward pass\n",
    "            test_loss += torch.sum((output - y_onehot)**2)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the maximum output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Get total number of correct samples\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Accuracy = Total Correct / Total Samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def seed(seed_value):\n",
    "\n",
    "    # This removes randomness, makes everything deterministic\n",
    "\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    use_cuda = False  # Set it to False if you are using a CPU\n",
    "    # Colab And Kaggle\n",
    "\n",
    "    seed(0)  # Used to fix randomness in the code! Very important!\n",
    "\n",
    "    # Convert the dataset to tensor and subtract the mean and divide by standard\n",
    "    # deviation. Why? So that neurons don't saturate!\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # ((mean), (std))    (x - mean)/std\n",
    "    ])\n",
    "    # [a1, a2, a3] / [b1, b2, b3] = [a1/b1, a2/b2, a3/b3]\n",
    "\n",
    "    # Wx + b =~ b\n",
    "\n",
    "    dataset1 = datasets.MNIST('./data', train=True, download=True, transform=transform)  # Get the train dataset\n",
    "    dataset2 = datasets.MNIST('./data', train=False, transform=transform)  # Get the test dataset\n",
    "\n",
    "    # Get the train data-loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, num_workers=6, shuffle=True, batch_size=64)\n",
    "    # Get the test data-loader\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, num_workers=6, shuffle=False, batch_size=1000)\n",
    "\n",
    "    model = Net()  # Get the model\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Put the model weights on GPU\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=10)  # Choose the optimizer and the set the learning rate\n",
    "\n",
    "    for epoch in range(1, 10 + 1):\n",
    "        train(model, use_cuda, train_loader, optimizer, epoch)  # Train the network\n",
    "        test(model, use_cuda, test_loader)  # Test the network\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist.pt\")\n",
    "\n",
    "    model.load_state_dict(torch.load('mnist.pt'))\n",
    "\n",
    "    # Loading a saved model - model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.274918\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.025267\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.020577\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.019472\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.015545\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.017731\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.013951\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.007282\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.011480\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.011476\n",
      "\n",
      "Test set: Average loss: 0.1049, Accuracy: 9394/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010957\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.008090\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.006146\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.011546\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007521\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.006160\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.008334\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.004962\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.010122\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.007099\n",
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 9536/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.009445\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.005759\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.005486\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003317\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.006060\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.003807\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.007417\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014120\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.005981\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008598\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9588/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005701\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.009952\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.003682\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005722\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003520\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006616\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.005776\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.009022\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004865\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.005905\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 9652/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010073\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.005446\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004753\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.008483\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002852\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.004889\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.007897\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005840\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.009440\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003051\n",
      "\n",
      "Test set: Average loss: 0.0545, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004560\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.007206\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.003539\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005356\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001615\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006138\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.007906\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002370\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.005132\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002808\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002659\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005936\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004451\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001209\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006464\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001330\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.007383\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001020\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004539\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001471\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005405\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002072\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001752\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000844\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002790\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006174\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.003062\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.003011\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003347\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003234\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002738\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004390\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.006352\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000903\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001409\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001507\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.004618\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002049\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004405\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000863\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006523\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001108\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000841\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002080\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002373\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002950\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.008099\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002576\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002516\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001436\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9740/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):  # Constructor\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 2 Layers\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "        # xavier_initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - shape = [64, 784]\n",
    "        # Layer 1\n",
    "        l1 = self.fc1(x)  # l1 - shape = [64, 100]\n",
    "\n",
    "        # Activation 1\n",
    "        l1_a1 = torch.sigmoid(l1)  # l1_a1 - shape = [64, 100]\n",
    "\n",
    "        # Layer 2\n",
    "        l2 = self.fc2(l1_a1)  # l2 - shape = [64, 10]\n",
    "\n",
    "        # Activation 2\n",
    "        l2_a2 = torch.sigmoid(l2)  # l2_a2 - shape = [64, 10]\n",
    "        \n",
    "        return l2_a2\n",
    "\n",
    "\n",
    "def train(model, use_cuda, train_loader, optimizer, epoch):\n",
    "\n",
    "    model.train()  # Tell the model to prepare for training\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Get the batch\n",
    "\n",
    "        # Converting the target to one-hot-encoding from categorical encoding\n",
    "        # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [64, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1\n",
    "\n",
    "        data = data.view([data.shape[0], 784])\n",
    "\n",
    "        if use_cuda:\n",
    "            data, y_onehot = data.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "        optimizer.zero_grad()  # Setting the cumulative gradients to 0\n",
    "        output = model(data)  # Forward pass through the model\n",
    "        loss = torch.mean((output - y_onehot)**2)  # Calculating the loss\n",
    "        loss.backward()  # Calculating the gradients of the model. Note that the model has not yet been updated.\n",
    "        optimizer.step()  # Updating the model parameters. Note that this does not remove the stored gradients!\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, use_cuda, test_loader):\n",
    "\n",
    "    model.eval()  # Tell the model to prepare for testing or evaluation\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Tell the model that gradients need not be calculated\n",
    "        for data, target in test_loader:  # Get the batch\n",
    "\n",
    "            # Converting the target to one-hot-encoding from categorical encoding\n",
    "            # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "            y_onehot = torch.zeros([target.shape[0], 10])\n",
    "            y_onehot[range(target.shape[0]), target] = 1\n",
    "            data = data.view([data.shape[0], 784])\n",
    "\n",
    "            if use_cuda:\n",
    "                data, target, y_onehot = data.cuda(), target.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "            # argmax([0.1, 0.2, 0.9, 0.4]) => 2\n",
    "            # output - shape = [1000, 10], argmax(dim=1) => [1000]\n",
    "            output = model(data)  # Forward pass\n",
    "            test_loss += torch.sum((output - y_onehot)**2)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the maximum output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Get total number of correct samples\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Accuracy = Total Correct / Total Samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def seed(seed_value):\n",
    "\n",
    "    # This removes randomness, makes everything deterministic\n",
    "\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    use_cuda = False  # Set it to False if you are using a CPU\n",
    "    # Colab And Kaggle\n",
    "\n",
    "    seed(0)  # Used to fix randomness in the code! Very important!\n",
    "\n",
    "    # Convert the dataset to tensor and subtract the mean and divide by standard\n",
    "    # deviation. Why? So that neurons don't saturate!\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # ((mean), (std))    (x - mean)/std\n",
    "    ])\n",
    "    # [a1, a2, a3] / [b1, b2, b3] = [a1/b1, a2/b2, a3/b3]\n",
    "\n",
    "    # Wx + b =~ b\n",
    "\n",
    "    dataset1 = datasets.MNIST('./data', train=True, download=True, transform=transform)  # Get the train dataset\n",
    "    dataset2 = datasets.MNIST('./data', train=False, transform=transform)  # Get the test dataset\n",
    "\n",
    "    # Get the train data-loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, num_workers=6, shuffle=True, batch_size=64)\n",
    "    # Get the test data-loader\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, num_workers=6, shuffle=False, batch_size=1000)\n",
    "\n",
    "    model = Net()  # Get the model\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Put the model weights on GPU\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=10)  # Choose the optimizer and the set the learning rate\n",
    "\n",
    "    for epoch in range(1, 10 + 1):\n",
    "        train(model, use_cuda, train_loader, optimizer, epoch)  # Train the network\n",
    "        test(model, use_cuda, test_loader)  # Test the network\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist.pt\")\n",
    "\n",
    "    model.load_state_dict(torch.load('mnist.pt'))\n",
    "\n",
    "    # Loading a saved model - model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.274918\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.025267\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.020577\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.019472\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.015545\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.017731\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.013951\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.007282\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.011480\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.011476\n",
      "\n",
      "Test set: Average loss: 0.1049, Accuracy: 9394/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010957\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.008090\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.006146\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.011546\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007521\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.006160\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.008334\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.004962\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.010122\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.007099\n",
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 9536/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.009445\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.005759\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.005486\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003317\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.006060\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.003807\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.007417\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014120\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.005981\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008598\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9588/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005701\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.009952\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.003682\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005722\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003520\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006616\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.005776\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.009022\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004865\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.005905\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 9652/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010073\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.005446\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004753\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.008483\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002852\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.004889\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.007897\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005840\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.009440\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003051\n",
      "\n",
      "Test set: Average loss: 0.0545, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004560\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.007206\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.003539\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005356\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001615\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006138\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.007906\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002370\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.005132\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002808\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002659\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005936\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004451\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001209\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006464\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001330\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.007383\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001020\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004539\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001471\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005405\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002072\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001752\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000844\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002790\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006174\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.003062\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.003011\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003347\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003234\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002738\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004390\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.006352\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000903\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001409\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001507\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.004618\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002049\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004405\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000863\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006523\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001108\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000841\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002080\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002373\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002950\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.008099\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002576\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002516\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001436\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9740/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):  # Constructor\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 2 Layers\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "        # xavier_initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - shape = [64, 784]\n",
    "        # Layer 1\n",
    "        l1 = self.fc1(x)  # l1 - shape = [64, 100]\n",
    "\n",
    "        # Activation 1\n",
    "        l1_a1 = torch.sigmoid(l1)  # l1_a1 - shape = [64, 100]\n",
    "\n",
    "        # Layer 2\n",
    "        l2 = self.fc2(l1_a1)  # l2 - shape = [64, 10]\n",
    "\n",
    "        # Activation 2\n",
    "        l2_a2 = torch.sigmoid(l2)  # l2_a2 - shape = [64, 10]\n",
    "        \n",
    "        return l2_a2\n",
    "\n",
    "\n",
    "def train(model, use_cuda, train_loader, optimizer, epoch):\n",
    "\n",
    "    model.train()  # Tell the model to prepare for training\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Get the batch\n",
    "\n",
    "        # Converting the target to one-hot-encoding from categorical encoding\n",
    "        # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [64, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1\n",
    "\n",
    "        data = data.view([data.shape[0], 784])\n",
    "\n",
    "        if use_cuda:\n",
    "            data, y_onehot = data.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "        optimizer.zero_grad()  # Setting the cumulative gradients to 0\n",
    "        output = model(data)  # Forward pass through the model\n",
    "        loss = torch.mean((output - y_onehot)**2)  # Calculating the loss\n",
    "        loss.backward()  # Calculating the gradients of the model. Note that the model has not yet been updated.\n",
    "        optimizer.step()  # Updating the model parameters. Note that this does not remove the stored gradients!\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, use_cuda, test_loader):\n",
    "\n",
    "    model.eval()  # Tell the model to prepare for testing or evaluation\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Tell the model that gradients need not be calculated\n",
    "        for data, target in test_loader:  # Get the batch\n",
    "\n",
    "            # Converting the target to one-hot-encoding from categorical encoding\n",
    "            # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "            y_onehot = torch.zeros([target.shape[0], 10])\n",
    "            y_onehot[range(target.shape[0]), target] = 1\n",
    "            data = data.view([data.shape[0], 784])\n",
    "\n",
    "            if use_cuda:\n",
    "                data, target, y_onehot = data.cuda(), target.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "            # argmax([0.1, 0.2, 0.9, 0.4]) => 2\n",
    "            # output - shape = [1000, 10], argmax(dim=1) => [1000]\n",
    "            output = model(data)  # Forward pass\n",
    "            test_loss += torch.sum((output - y_onehot)**2)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the maximum output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Get total number of correct samples\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Accuracy = Total Correct / Total Samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def seed(seed_value):\n",
    "\n",
    "    # This removes randomness, makes everything deterministic\n",
    "\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    use_cuda = False  # Set it to False if you are using a CPU\n",
    "    # Colab And Kaggle\n",
    "\n",
    "    seed(0)  # Used to fix randomness in the code! Very important!\n",
    "\n",
    "    # Convert the dataset to tensor and subtract the mean and divide by standard\n",
    "    # deviation. Why? So that neurons don't saturate!\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # ((mean), (std))    (x - mean)/std\n",
    "    ])\n",
    "    # [a1, a2, a3] / [b1, b2, b3] = [a1/b1, a2/b2, a3/b3]\n",
    "\n",
    "    # Wx + b =~ b\n",
    "\n",
    "    dataset1 = datasets.MNIST('./data', train=True, download=True, transform=transform)  # Get the train dataset\n",
    "    dataset2 = datasets.MNIST('./data', train=False, transform=transform)  # Get the test dataset\n",
    "\n",
    "    # Get the train data-loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, num_workers=6, shuffle=True, batch_size=64)\n",
    "    # Get the test data-loader\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, num_workers=6, shuffle=False, batch_size=1000)\n",
    "\n",
    "    model = Net()  # Get the model\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Put the model weights on GPU\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=10)  # Choose the optimizer and the set the learning rate\n",
    "\n",
    "    for epoch in range(1, 10 + 1):\n",
    "        train(model, use_cuda, train_loader, optimizer, epoch)  # Train the network\n",
    "        test(model, use_cuda, test_loader)  # Test the network\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist.pt\")\n",
    "\n",
    "    model.load_state_dict(torch.load('mnist.pt'))\n",
    "\n",
    "    # Loading a saved model - model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.274918\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.025267\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.020577\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.019472\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.015545\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.017731\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.013951\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.007282\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.011480\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.011476\n",
      "\n",
      "Test set: Average loss: 0.1049, Accuracy: 9394/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010957\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.008090\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.006146\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.011546\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007521\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.006160\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.008334\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.004962\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.010122\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.007099\n",
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 9536/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.009445\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.005759\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.005486\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003317\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.006060\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.003807\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.007417\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014120\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.005981\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008598\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9588/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005701\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.009952\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.003682\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005722\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003520\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006616\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.005776\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.009022\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004865\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.005905\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 9652/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010073\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.005446\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004753\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.008483\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002852\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.004889\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.007897\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005840\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.009440\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003051\n",
      "\n",
      "Test set: Average loss: 0.0545, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004560\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.007206\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.003539\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005356\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001615\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006138\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.007906\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002370\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.005132\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002808\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002659\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005936\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004451\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001209\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006464\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001330\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.007383\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001020\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004539\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001471\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005405\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002072\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001752\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000844\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002790\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006174\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.003062\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.003011\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003347\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003234\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002738\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004390\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.006352\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000903\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001409\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001507\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.004618\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002049\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004405\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000863\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006523\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001108\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000841\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002080\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002373\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002950\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.008099\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002576\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002516\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001436\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9740/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):  # Constructor\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 2 Layers\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "        # xavier_initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - shape = [64, 784]\n",
    "        # Layer 1\n",
    "        l1 = self.fc1(x)  # l1 - shape = [64, 100]\n",
    "\n",
    "        # Activation 1\n",
    "        l1_a1 = torch.sigmoid(l1)  # l1_a1 - shape = [64, 100]\n",
    "\n",
    "        # Layer 2\n",
    "        l2 = self.fc2(l1_a1)  # l2 - shape = [64, 10]\n",
    "\n",
    "        # Activation 2\n",
    "        l2_a2 = torch.sigmoid(l2)  # l2_a2 - shape = [64, 10]\n",
    "        \n",
    "        return l2_a2\n",
    "\n",
    "\n",
    "def train(model, use_cuda, train_loader, optimizer, epoch):\n",
    "\n",
    "    model.train()  # Tell the model to prepare for training\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Get the batch\n",
    "\n",
    "        # Converting the target to one-hot-encoding from categorical encoding\n",
    "        # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [64, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1\n",
    "\n",
    "        data = data.view([data.shape[0], 784])\n",
    "\n",
    "        if use_cuda:\n",
    "            data, y_onehot = data.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "        optimizer.zero_grad()  # Setting the cumulative gradients to 0\n",
    "        output = model(data)  # Forward pass through the model\n",
    "        loss = torch.mean((output - y_onehot)**2)  # Calculating the loss\n",
    "        loss.backward()  # Calculating the gradients of the model. Note that the model has not yet been updated.\n",
    "        optimizer.step()  # Updating the model parameters. Note that this does not remove the stored gradients!\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, use_cuda, test_loader):\n",
    "\n",
    "    model.eval()  # Tell the model to prepare for testing or evaluation\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Tell the model that gradients need not be calculated\n",
    "        for data, target in test_loader:  # Get the batch\n",
    "\n",
    "            # Converting the target to one-hot-encoding from categorical encoding\n",
    "            # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "            y_onehot = torch.zeros([target.shape[0], 10])\n",
    "            y_onehot[range(target.shape[0]), target] = 1\n",
    "            data = data.view([data.shape[0], 784])\n",
    "\n",
    "            if use_cuda:\n",
    "                data, target, y_onehot = data.cuda(), target.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "            # argmax([0.1, 0.2, 0.9, 0.4]) => 2\n",
    "            # output - shape = [1000, 10], argmax(dim=1) => [1000]\n",
    "            output = model(data)  # Forward pass\n",
    "            test_loss += torch.sum((output - y_onehot)**2)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the maximum output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Get total number of correct samples\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Accuracy = Total Correct / Total Samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def seed(seed_value):\n",
    "\n",
    "    # This removes randomness, makes everything deterministic\n",
    "\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    use_cuda = False  # Set it to False if you are using a CPU\n",
    "    # Colab And Kaggle\n",
    "\n",
    "    seed(0)  # Used to fix randomness in the code! Very important!\n",
    "\n",
    "    # Convert the dataset to tensor and subtract the mean and divide by standard\n",
    "    # deviation. Why? So that neurons don't saturate!\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # ((mean), (std))    (x - mean)/std\n",
    "    ])\n",
    "    # [a1, a2, a3] / [b1, b2, b3] = [a1/b1, a2/b2, a3/b3]\n",
    "\n",
    "    # Wx + b =~ b\n",
    "\n",
    "    dataset1 = datasets.MNIST('./data', train=True, download=True, transform=transform)  # Get the train dataset\n",
    "    dataset2 = datasets.MNIST('./data', train=False, transform=transform)  # Get the test dataset\n",
    "\n",
    "    # Get the train data-loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, num_workers=6, shuffle=True, batch_size=64)\n",
    "    # Get the test data-loader\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, num_workers=6, shuffle=False, batch_size=1000)\n",
    "\n",
    "    model = Net()  # Get the model\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Put the model weights on GPU\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=10)  # Choose the optimizer and the set the learning rate\n",
    "\n",
    "    for epoch in range(1, 10 + 1):\n",
    "        train(model, use_cuda, train_loader, optimizer, epoch)  # Train the network\n",
    "        test(model, use_cuda, test_loader)  # Test the network\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist.pt\")\n",
    "\n",
    "    model.load_state_dict(torch.load('mnist.pt'))\n",
    "\n",
    "    # Loading a saved model - model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
